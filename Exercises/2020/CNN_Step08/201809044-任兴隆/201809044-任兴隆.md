# 第17章 卷积神经网络原理

## 17.0 卷积神经网络概述

###  卷积神经网络的能力

卷积神经网络（CNN，Convolutional Neural Net)是神经网络的类型之一，在图像识别和分类领域中取得了非常好的效果，比如识别人脸、物体、交通标识等，这就为机器人、自动驾驶等应用提供了坚实的技术基础。

在下面图中卷积神经网络展现了识别人类日常生活中的各种物体的能力。

![1](1.png)

 识别出四个人在一条船上

![2](2.png)

识别出一个农场中的车、狗、马、人等物体

下面的场景要比前一个更有趣一些。

![3](3.png)

两匹斑马

![4](4.png)

两个骑车人

卷积神经网络可以识别出上面两张图中的物体和场景，图17-3$^{[2]}$是“两匹斑马站在泥地上”，图17-4$^{[2]}$是“一个在道路上骑车的男人旁边还有个女人”。当然，识别物体和给出简要的场景描述是两套系统配合才能完成的任务，第一个系统只负责识别，第二个系统可以根据第一个系统的输出形成摘要文字。

###  卷积神经网络的典型结构

一个典型的卷积神经网络的结构如图17-5所示。

![5](5.png)

图5 卷积神经网络的典型结构图

我们分析一下它的层级结构：

1. 原始的输入是一张图片，可以是彩色的，也可以是灰度的或黑白的。这里假设是只有一个通道的图片，目的是识别0~9的手写体数字；
2. 第一层卷积，我们使用了4个卷积核，得到了4张feature map；激活函数层没有单独画出来，这里我们紧接着卷积操作使用了Relu激活函数；
3. 第二层是池化，使用了Max Pooling方式，把图片的高宽各缩小一倍，但仍然是4个feature map；
4. 第三层卷积，我们使用了4x6个卷积核，其中4对应着输入通道，6对应着输出通道，从而得到了6张feature map，当然也使用了Relu激活函数；
5. 第四层再次做一次池化，现在得到的图片尺寸只是原始尺寸的四分之一左右；
6. 第五层把第四层的6个图片展平成一维，成为一个fully connected层；
7. 第六层再接一个小一些的fully connected层；
8. 最后接一个softmax函数，判别10个分类。

所以，在一个典型的卷积神经网络中，会至少包含以下几个层：

- 卷积层
- 激活函数层
- 池化层
- 全连接分类层

我们会在后续的小节中讲解卷积层和池化层的具体工作原理。

###  卷积核的作用

我们遇到了一个新的概念：卷积核。卷积网络之所以能工作，完全是卷积核的功劳。什么是卷积核呢？卷积核其实就是一个小矩阵，类似这样：

```
1.1  0.23  -0.45
0.1  -2.1   1.24
0.74 -1.32  0.01
```

这是一个3x3的卷积核，还会有1x1、5x5、7x7、9x9、11x11的卷积核。在卷积层中，我们会用输入数据与卷积核相乘，得到输出数据，就类似全连接层中的Weights一样，所以卷积核里的数值，也是通过反向传播的方法学习到的。

下面我们看看卷积核的具体作用。

![6](6.png)

图6 卷积核的作用

表17-2 各个卷积核的作用

|序号|名称|说明|
|---|---|---|
|1|锐化|如果一个像素点比周围像素点亮，则此算子会令其更亮|
|2|检测竖边|检测出了十字线中的竖线，由于是左侧和右侧分别检查一次，所以得到两条颜色不一样的竖线|
|3|周边|把周边增强，把同色的区域变弱，形成大色块|
|4|Sobel-Y|纵向亮度差分可以检测出横边，与横边检测不同的是，它可以使得两条横线具有相同的颜色，具有分割线的效果|
|5|Identity|中心为1四周为0的过滤器，卷积后与原图相同|
|6|横边检测|检测出了十字线中的横线，由于是上侧和下侧分别检查一次，所以得到两条颜色不一样的横线|
|7|模糊|通过把周围的点做平均值计算而“杀富济贫”造成模糊效果|
|8|Sobel-X|横向亮度差分可以检测出竖边，与竖边检测不同的是，它可以使得两条竖线具有相同的颜色，具有分割线的效果|
|9|浮雕|形成大理石浮雕般的效果|

###  卷积后续的运算

前面我们认识到了卷积核的强大能力，卷积神经网络通过反向传播而令卷积核自我学习，找到分布在图片中的不同的feature，最后形成的卷积核中的数据。但是如果想达到这种效果，只有卷积层的话是不够的，还需要激活函数、池化等操作的配合。

图17-7中的四个子图，依次展示了：

1. 原图
2. 卷积结果
3. 激活结果
4. 池化结果

![7](7.png)

图7 原图经过卷积-激活-池化操作后的效果

1. 注意图一是原始图片，用cv2读取出来的图片，其顺序是反向的，即：

- 第一维是高度
- 第二维是宽度
- 第三维是彩色通道数，但是其顺序为BGR，而不是常用的RGB

1. 我们对原始图片使用了一个3x1x3x3的卷积核，因为原始图片为彩色图片，所以第一个维度是3，对应RGB三个彩色通道；我们希望只输出一张feature map，以便于说明，所以第二维是1；我们使用了3x3的卷积核，用的是sobel x算子。所以图二是卷积后的结果。

2. 图三做了一层Relu激活计算，把小于0的值都去掉了，只留下了一些边的特征。

3. 图四是图三的四分之一大小，虽然图片缩小了，但是特征都没有丢失，反而因为图像尺寸变小而变得密集，亮点的密度要比图三大而粗。

###  卷积神经网络的学习

从17.0.2节中的整体图中，可以看到在卷积-池化等一些列操作的后面，要接全连接层，这里的全连接层和我们在前面学习的深度网络的功能一模一样，都是做为分类层使用。

在最后一层的池化后面，把所有特征数据变成一个一维的全连接层，然后就和普通的深度全连接网络一样了，通过在最后一层的softmax分类函数，以及多分类交叉熵函数，对比图片的OneHot编码标签，回传误差值，从全连接层传回到池化层，通过激活函数层再回传给卷积层，对卷积核的数值进行梯度更新，实现卷积核数值的自我学习。

但是这里有个问题，回忆一下MNIST数据集，所有的样本数据都是处于28x28方形区域的中间地带，如下图中的左上角的图片A所示。

![8](8.png)

图8 同一个背景下数字8的大小、位置、形状的不同

我们的问题是：

1. 如果这个“8”的位置很大地偏移到了右下角，使得左侧留出来一大片空白，即发生了平移，如上图右上角子图B
2. “8”做了一些旋转或者翻转，即发生了旋转视角，如上图左下角子图C
3. “8”缩小了很多或放大了很多，即发生了尺寸变化，如上图右下角子图D

尽管发生了变化，但是对于人类的视觉系统来说都可以轻松应对，即平移不变性、旋转视角不变性、尺度不变性。那么卷积神经网络网络如何处理呢？

- 平移不变性
  
  对于原始图A，平移后得到图B，对于同一个卷积核来说，都会得到相同的特征，这就是卷积核的权值共享。但是特征处于不同的位置，由于距离差距较大，即使经过多层池化后，也不能处于近似的位置。此时，后续的全连接层会通过权重值的调整，把这两个相同的特征看作同一类的分类标准之一。如果是小距离的平移，通过池化层就可以处理了。

- 旋转不变性

  对于原始图A，有小角度的旋转得到C，卷积层在A图上得到特征a，在C图上得到特征c，可以想象a与c的位置间的距离不是很远，在经过两层池化以后，基本可以重合。所以卷积网络对于小角度旋转是可以容忍的，但是对于较大的旋转，需要使用数据增强来增加训练样本。一个极端的例子是当6旋转90度时，谁也不能确定它到底是6还是9。

- 尺度不变性

  对于原始图A和缩小的图D，人类可以毫不费力地辨别出它们是同一个东西。池化在这里是不是有帮助呢？没有！因为神经网络对A做池化的同时，也会用相同的方法对D做池化，这样池化的次数一致，最终D还是比A小。如果我们有多个卷积视野，相当于从两米远的地方看图A，从一米远的地方看图D，那么A和D就可以很相近似了。这就是Inception的想法，用不同尺寸的卷积核去同时寻找同一张图片上的特征。

代码如下：
 #Copyright (c) Microsoft. All rights reserved.
 #Licensed under the MIT license. See LICENSE file in the project root for full license information.

from matplotlib import pyplot as plt
import numpy as np
import cv2

from MiniFramework.ConvWeightsBias import *
from MiniFramework.ConvLayer import *
from MiniFramework.ActivationLayer import *
from MiniFramework.PoolingLayer import *
from MiniFramework.HyperParameters_4_2 import *
from MiniFramework.jit_utility import *

circle_pic = "circle.png"

def normalize(x, max_value=1):
    min = np.min(x)
    max = np.max(x)
    x_n = (x - min)/(max - min)*max_value
    return x_n

def try_filters(file_name):
    img = cv2.imread(file_name)
    # cv2 format is:G B R, change it to R G B
    img1=img[:,:,[2,1,0]]
    #plt.imshow(img2)
    #plt.show()
    img2 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)
    batch_size = 1
    input_channel = 1
    (height, width) = img2.shape
    FH = 3
    FW = 3
    print(img2.shape)
    data = img2.reshape((1,1,height,width))
    hp = HyperParameters_4_2(
        0.1, 10, batch_size,
        net_type=NetType.MultipleClassifier,
        init_method=InitialMethod.Xavier,
        optimizer_name=OptimizerName.Momentum)
    conv = ConvLayer((1,height,width), (1,FH,FW), (1,1), hp)
    conv.initialize("know_cnn", "name")
    
    filters = [
        np.array([0,-1,0,
                  -1,5,-1,
                  0,-1,0]),         # sharpness filter
        np.array([0,0,0,
                  -1,2,-1,
                  0,0,0]),          # vertical edge
        np.array([1,1,1,
                  1,-9,1,
                  1,1,1]),          # surround
        np.array([-1,-2,-1,
                  0,0,0,
                  1,2,1]),          # sobel y
        np.array([0,0,0,
                  0,1,0,
                  0,0,0]),          # nothing
        np.array([0,-1,0,
                  0,2,0,
                  0,-1,0]),         # horizontal edge
        np.array([0.11,0.11,0.11,
                  0.11,0.11,0.11,
                  0.11,0.11,0.11]), # blur
        np.array([-1,0,1,
                  -2,0,2,
                  -1,0,1]),         # sobel x
        np.array([2,0,0,
                  0,-1,0,
                  0,0,-1])]         # embossing

    filters_name = ["sharpness", "vertical edge", "surround", "sobel y", "nothing", "horizontal edge", "blur", "sobel x", "embossing"]

    fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(9,9))
    for i in range(len(filters)):
        filter = np.repeat(filters[i], input_channel).reshape(batch_size, input_channel,FH,FW)
        conv.set_filter(filter, None)
        z = conv.forward(data)
        #z = normalize(z, 255)
        ax[i//3, i%3].imshow(z[0,0])
        ax[i//3, i%3].set_title(filters_name[i])
        ax[i//3, i%3].axis("off")
    plt.suptitle("filters")
    plt.show()
    return z

def conv_relu_pool():
    img = cv2.imread(circle_pic)
    #img2 = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    batch_size = 1
    (height, width, input_channel) = img.shape
    FH = 3
    FW = 3
    data = np.transpose(img, axes=(2,1,0)).reshape((batch_size,input_channel,width,height))
    hp = HyperParameters_4_2(
        0.1, 10, batch_size,
        net_type=NetType.MultipleClassifier,
        init_method=InitialMethod.Xavier,
        optimizer_name=OptimizerName.Momentum)
    conv = ConvLayer((input_channel,width,height),(1,FH,FW),(1,0),hp)
    conv.initialize("know_cnn", "conv")
    kernal = np.array([ -1,0,1,
                        -2,0,2,
                        -1,0,1])
    filter = np.repeat(kernal, input_channel).reshape(batch_size, input_channel,FH,FW)
    conv.set_filter(filter, None)
    z1 = conv.forward(data)
    z2 = Relu().forward(z1)
    pool = PoolingLayer(z2[0].shape, (2,2), 2, PoolingTypes.MAX)
    pool.initialize("know_cnn", "pool")
    z3 = pool.forward(z2)

    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,6))
    ax[0,0].imshow(img[:,:,[2,1,0]])
    ax[0,0].axis("off")
    ax[0,0].set_title("source:" + str(img.shape))
    ax[0,1].imshow(z1[0,0].T)
    ax[0,1].axis("off")
    ax[0,1].set_title("conv:" + str(z1.shape))
    ax[1,0].imshow(z2[0,0].T)
    ax[1,0].axis("off")
    ax[1,0].set_title("relu:" + str(z2.shape))
    ax[1,1].imshow(z3[0,0].T)
    ax[1,1].axis("off")
    ax[1,1].set_title("pooling:" + str(z3.shape))

    plt.suptitle("conv-relu-pool")
    plt.show()

    
if __name__ == '__main__':
    try_filters(circle_pic)
    conv_relu_pool()

运行结果如下：
![9](9.png)
![10](10.png)


level2代码如下：
 #Copyright (c) Microsoft. All rights reserved.
 #Licensed under the MIT license. See LICENSE file in the project root for full license information.

import numpy
import numba
import time

from MiniFramework.ConvWeightsBias import *
from MiniFramework.ConvLayer import *
from MiniFramework.HyperParameters_4_2 import *

def calculate_output_size(input_h, input_w, filter_h, filter_w, padding, stride=1):
    output_h = (input_h - filter_h + 2 * padding) // stride + 1    
    output_w = (input_w - filter_w + 2 * padding) // stride + 1
    return (output_h, output_w)

def test_2d_conv():
    batch_size = 1
    stride = 1
    padding = 0
    fh = 2
    fw = 2
    input_channel = 1
    output_channel = 1
    iw = 3
    ih = 3
    (output_height, output_width) = calculate_output_size(ih, iw, fh, fw, padding, stride)
    wb = ConvWeightsBias(output_channel, input_channel, fh, fw, InitialMethod.MSRA, OptimizerName.SGD, 0.1)
    wb.Initialize("test", "test", True)
    wb.W = np.array([3,2,1,0]).reshape(1,1,2,2)
    wb.B = np.array([0])
    x = np.array(range(9)).reshape(1,1,3,3)
    output1 = jit_conv_4d(x, wb.W, wb.B, output_height, output_width, stride)
    print("input=\n", x)
    print("weights=\n", wb.W)
    print("output=\n", output1)

    col = img2col(x, 2, 2, 1, 0)
    w = wb.W.reshape(4, 1)
    output2 = np.dot(col, w)
    print("input=\n", col)
    print("weights=\n", w)
    print("output2=\n", output2)


def test_4d_im2col():
    batch_size = 2
    stride = 1
    padding = 0
    fh = 2
    fw = 2
    input_channel = 3
    output_channel = 2
    iw = 3
    ih = 3

    x = np.random.randn(batch_size, input_channel, iw, ih)
    params = HyperParameters_4_2(
        0.1, 1, batch_size,
        net_type=NetType.MultipleClassifier,
        init_method=InitialMethod.Xavier)
    c1 = ConvLayer((input_channel,iw,ih), (output_channel,fh,fw), (stride, padding), params)
    c1.initialize("test", "test", False)
    f1 = c1.forward_numba(x)
    f2 = c1.forward_img2col(x)
    print("correctness:", np.allclose(f1, f2, atol=1e-7))

def understand_4d_im2col():
    batch_size = 2
    stride = 1
    padding = 0
    fh = 2
    fw = 2
    input_channel = 3
    output_channel = 2
    iw = 3
    ih = 3
    (output_height, output_width) = calculate_output_size(ih, iw, fh, fw, padding, stride)
    wb = ConvWeightsBias(output_channel, input_channel, fh, fw, InitialMethod.MSRA, OptimizerName.SGD, 0.1)
    wb.Initialize("test", "test", True)
    wb.W = np.array(range(output_channel * input_channel * fh * fw)).reshape(output_channel, input_channel, fh, fw)
    wb.B = np.array([0])
    x = np.array(range(input_channel * iw * ih * batch_size)).reshape(batch_size, input_channel, ih, iw)

    col = img2col(x, 2, 2, 1, 0)
    w = wb.W.reshape(output_channel, -1).T
    output = np.dot(col, w)
    print("x=\n", x)
    print("col_x=\n", col)
    print("weights=\n", wb.W)
    print("col_w=\n", w)
    print("output=\n", output)
    out2 = output.reshape(batch_size, output_height, output_width, -1)
    print("out2=\n", out2)
    out3 = np.transpose(out2, axes=(0, 3, 1, 2))
    print("conv result=\n", out3)


def test_performance():
    batch_size = 64
    params = HyperParameters_4_2(
        0.1, 1, batch_size,
        net_type=NetType.MultipleClassifier,
        init_method=InitialMethod.Xavier)
    stride = 1
    padding = 1
    fh = 3
    fw = 3
    input_channel = 3
    output_channel = 4
    iw = 28
    ih = 28
    # 64 个 3 x 28 x 28 的图像输入（模拟 mnist）
    x = np.random.randn(batch_size, input_channel, iw, ih)
    
    c1 = ConvLayer((input_channel,iw,ih), (output_channel,fh,fw), (stride, padding), params)
    c1.initialize("test", "test", False)

    # dry run
    for i in range(5):
        f1 = c1.forward_numba(x)
        delta_in = np.ones((f1.shape))
        #b1, dw1, db1 = c1.backward_numba(delta_in, 1)
    # run
    s1 = time.time()
    for i in range(1000):
        f1 = c1.forward_numba(x)
        #b1, dw1, db1 = c1.backward_numba(delta_in, 1)
    e1 = time.time()
    print("method numba:", e1-s1)

    # dry run
    for i in range(5):
        f2 = c1.forward_img2col(x)
        #b2, dw2, db2 = c1.backward_col2img(delta_in, 1)
    # run
    s2 = time.time()
    for i in range(1000):
        f2 = c1.forward_img2col(x)
        #b2, dw2, db2 = c1.backward_col2img(delta_in, 1)
    e2 = time.time()
    print("method img2col:", e2-s2)

    print("compare correctness of method 1 and method 2:")
    print("forward:", np.allclose(f1, f2, atol=1e-7))
    #print("backward:", np.allclose(b1, b2, atol=1e-7))
    #print("dW:", np.allclose(dw1, dw2, atol=1e-7))
    #print("dB:", np.allclose(db1, db2, atol=1e-7))

if __name__ == '__main__':
    test_2d_conv()
    understand_4d_im2col()
    test_4d_im2col()
    test_performance()

运行截图：
![25](25.png)


## 17.1 卷积的前向计算

###  卷积的数学定义

#### 连续定义

$$h(x)=(f*g)(x) = \int_{-\infty}^{\infty} f(t)g(x-t)dt \tag{1}$$

卷积与傅里叶变换有着密切的关系。利用这点性质，即两函数的傅里叶变换的乘积等于它们卷积后的傅里叶变换，能使傅里叶分析中许多问题的处理得到简化。

#### 离散定义

$$h(x) = (f*g)(x) = \sum^{\infty}_{t=-\infty} f(t)g(x-t) \tag{2}$$

###  一维卷积实例

有两枚骰子$f,g$，掷出后二者相加为4的概率如何计算？

第一种情况：$f(1)g(3), 3+1=4$，如图11所示

![11](11.png)

图11 第一种情况

第二种情况：$f(2)g(2), 2+2=4$，如图12所示。

![12](12.png)

图12 第二种情况

第三种情况：$f(3)g(1), 1+3=4$，如图13所示。

![13](13.png)

图13 第三种情况

因此，两枚骰子点数加起来为4的概率为：

$$
\begin{aligned}
h(4) &= f(1)g(3)+f(2)g(2)+f(3)g(1) \\\\
&=f(1)g(4-1) + f(2)g(4-2) + f(3)g(4-3)
\end{aligned}
$$

符合卷积的定义，把它写成标准的形式就是公式2：

$$h(4)=(f*g)(4)=\sum _{t=1}^{3}f(t)g(4-t)$$

###  单入单出的二维卷积

二维卷积一般用于图像处理上。在二维图片上做卷积，如果把图像Image简写为$I$，把卷积核Kernal简写为$K$，则目标图片的第$(i,j)$个像素的卷积值为：

$$
h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(m,n)K(i-m,j-n) \tag{3}
$$

可以看出，这和一维情况下的公式2是一致的。从卷积的可交换性，我们可以把公式3等价地写作：

$$
h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(i-m,j-n)K(m,n) \tag{4}
$$

公式4的成立，是因为我们将Kernal进行了翻转。在神经网络中，一般会实现一个互相关函数(corresponding function)，而卷积运算几乎一样，但不反转Kernal：

$$
h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(i+m,j+n)K(m,n) \tag{5}
$$

在图像处理中，自相关函数和互相关函数定义如下：

- 自相关：设原函数是f(t)，则$h=f(t) \star f(-t)$，其中$\star$表示卷积
- 互相关：设两个函数分别是f(t)和g(t)，则$h=f(t) \star g(-t)$

互相关函数的运算，是两个序列滑动相乘，两个序列都不翻转。卷积运算也是滑动相乘，但是其中一个序列需要先翻转，再相乘。所以，从数学意义上说，机器学习实现的是互相关函数，而不是原始含义上的卷积。但我们为了简化，把公式5也称作为卷积。这就是卷积的来源。

结论：

1. 我们实现的卷积操作不是原始数学含义的卷积，而是工程上的卷积，可以简称为卷积
2. 在实现卷积操作时，并不会反转卷积核

在传统的图像处理中，卷积操作多用来进行滤波，锐化或者边缘检测啥的。我们可以认为卷积是利用某些设计好的参数组合（卷积核）去提取图像空域上相邻的信息。

按照公式5，我们可以在4x4的图片上，用一个3x3的卷积核，通过卷积运算得到一个2x2的图片，运算的过程如图17-12所示。

![14](14.png)
图14 卷积运算的过程

###  单入多出的升维卷积

原始输入是一维的图片，但是我们可以用多个卷积核分别对其计算，从而得到多个特征输出。如图15所示。

![15](15.png)

图15 单入多出的升维卷积

一张4x4的图片，用两个卷积核并行地处理，输出为2个2x2的图片。在训练过程中，这两个卷积核会完成不同的特征学习。

### 多入单出的降维卷积

一张图片，通常是彩色的，具有红绿蓝三个通道。我们可以有两个选择来处理：

1. 变成灰度的，每个像素只剩下一个值，就可以用二维卷积
2. 对于三个通道，每个通道都使用一个卷积核，分别处理红绿蓝三种颜色的信息

显然第2种方法可以从图中学习到更多的特征，于是出现了三维卷积，即有三个卷积核分别对应书的三个通道，三个子核的尺寸是一样的，比如都是2x2，这样的话，这三个卷积核就是一个3x2x2的立体核，称为过滤器Filter，所以称为三维卷积。

![16](16.png)

图16 多入单出的降维卷积

在上图中，每一个卷积核对应着左侧相同颜色的输入通道，三个过滤器的值并不一定相同。对三个通道各自做卷积后，得到右侧的三张特征图，然后再按照原始值不加权地相加在一起，得到最右侧的白色特征图，这张图里面已经把三种颜色的特征混在一起了，所以画成了白色，表示没有颜色特征了。

虽然输入图片是多个通道的，或者说是三维的，但是在相同数量的过滤器的计算后，相加在一起的结果是一个通道，即2维数据，所以称为降维。这当然简化了对多通道数据的计算难度，但同时也会损失多通道数据自带的颜色信息。

###  多入多出的同维卷积

在上面的例子中，是一个过滤器Filter内含三个卷积核Kernal。我们假设有一色图片为3x3的，如果有两组3x2x2的卷积核的话，会做什么样的卷积计算？看图17-15。

![17](17.png)

图17 多入多出的卷积运算

第一个过滤器Filter-1为棕色所示，它有三卷积核(Kernal)，命名为Kernal-1，Keanrl-2，Kernal-3，分别在红绿蓝三个输入通道上进行卷积操作，生成三个2x2的输出Feature-1,n。然后三个Feature-1,n相加，并再加上b1偏移值，形成最后的棕色输出Result-1。

对于灰色的过滤器Filter-2也是一样，先生成三个Feature-2,n，然后相加再加b2，最后得到Result-2。

之所以Feature-m,n还用红绿蓝三色表示，是因为在此时，它们还保留着红绿蓝三种色彩的各自的信息，一旦相加后得到Result，这种信息就丢失了。

代码如下：
# Copyright (c) Microsoft. All rights reserved.
# Licensed under the MIT license. See LICENSE file in the project root for full license information.

import numpy as np
from MiniFramework.jit_utility import *
from matplotlib import pyplot as plt
import cv2

circle_pic = "circle.png"

def normalize(x):
    min = np.min(x)
    max = np.max(x)
    x_n = (x - min)/(max - min)
    return x_n

def create_zero_array(x,w):
    out_h, out_w = calculate_output_size(x.shape[0], x.shape[1], w.shape[0], w.shape[1], 0, 1)
    output = np.zeros((out_h, out_w))
    return output

def train(x, w, b, y):
    output = create_zero_array(x, w)
    for i in range(10000):
        # forward
        jit_conv_2d(x, w, b, output)
        # loss
        t1 = (output - y)
        m = t1.shape[0]*t1.shape[1]
        LOSS = np.multiply(t1, t1)
        loss = np.sum(LOSS)/2/m
        print(i,loss)
        if loss < 1e-7:
            break
        # delta
        delta = output - y
        # backward
        dw = np.zeros(w.shape)
        jit_conv_2d(x, delta, b, dw)
        w = w - 0.5 * dw/m
    #end for
    return w


def create_sample_image():
    img_color = cv2.imread(circle_pic)
    img_gray = normalize(cv2.cvtColor(img_color, cv2.COLOR_RGB2GRAY))
    w = np.array([[0,-1,0],
                  [0, 2,0],
                  [0,-1,0]])
    b = 0
    y = create_zero_array(img_gray, w)
    jit_conv_2d(img_gray, w, b, y)
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(6,4))
    ax[0].imshow(img_gray, cmap='gray')
    ax[0].set_title("source")
    ax[1].imshow(y, cmap='gray')
    ax[1].set_title("target")
    plt.show()
    return img_gray, w, y

def show_result(img_gray, w_true, w_result):
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(6,4))
    y = create_zero_array(img_gray, w_true)
    jit_conv_2d(img_gray, w_true, 0, y)
    ax[0].imshow(y, cmap='gray')
    ax[0].set_title("true")
    z = create_zero_array(img_gray, w_result)
    jit_conv_2d(img_gray, w_result, 0, z)
    ax[1].imshow(z, cmap='gray')
    ax[1].set_title("result")
    plt.show()

if __name__ == '__main__':
    # 创建样本数据
    x, w_true, y = create_sample_image()
    # 随机初始化卷积核
    w_init = np.random.normal(0, 0.1, w_true.shape)
    # 训练
    w_result = train(x,w_init,0,y)
    # 打印比较真实卷积核值和训练出来的卷积核值
    print("w_true:\n", w_true)
    print("w_result:\n", w_result)
    # 用训练出来的卷积核值对原始图片进行卷积
    y_hat = np.zeros(y.shape)
    jit_conv_2d(x, w_true, 0, y_hat)
    # 与真实的卷积核的卷积结果比较
    show_result(x, w_true, w_result)
    # 比较卷积核值的差异核卷积结果的差异
    print("w allclose:", np.allclose(w_true, w_result, atol=1e-2))
    print("y allclose:", np.allclose(y, y_hat, atol=1e-2))

运行截图如下：
![20](20.png)

## 17.2 计算反向传播的梯度矩阵

正向公式：

$$Z = W*A+b \tag{0}$$

其中，W是卷积核，*表示卷积（互相关）计算，A为当前层的输入项，b是偏移（未在图中画出），Z为当前层的输出项，但尚未经过激活函数处理。

我们举一个具体的例子便于分析。图18是正向计算过程。

![18](18.png)

图18 卷积正向运算

分解到每一项就是下列公式：

$$z_{11} = w_{11} \cdot a_{11} + w_{12} \cdot a_{12} + w_{21} \cdot a_{21} + w_{22} \cdot a_{22} + b \tag{1}$$
$$z_{12} = w_{11} \cdot a_{12} + w_{12} \cdot a_{13} + w_{21} \cdot a_{22} + w_{22} \cdot a_{23} + b \tag{2}$$
$$z_{21} = w_{11} \cdot a_{21} + w_{12} \cdot a_{22} + w_{21} \cdot a_{31} + w_{22} \cdot a_{32} + b \tag{3}$$
$$z_{22} = w_{11} \cdot a_{22} + w_{12} \cdot a_{23} + w_{21} \cdot a_{32} + w_{22} \cdot a_{33} + b \tag{4}$$

求损失函数$J$对$a_{11}$的梯度：

$$
\frac{\partial J}{\partial a_{11}}=\frac{\partial J}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}=\delta_{z11}\cdot w_{11} \tag{5}
$$

上式中，$\delta_{z11}$是从网络后端回传到本层的$z_{11}$单元的梯度。

求$J$对$a_{12}$的梯度时，先看正向公式，发现$a_{12}$对$z_{11}$和$z_{12}$都有贡献，因此需要二者的偏导数相加：

$$
\frac{\partial J}{\partial a_{12}}=\frac{\partial J}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{12}}+\frac{\partial J}{\partial z_{12}} \frac{\partial z_{12}}{\partial a_{12}}=\delta_{z11} \cdot w_{12}+\delta_{z12} \cdot w_{11} \tag{6}
$$

最复杂的是求$a_{22}$的梯度，因为从正向公式看，所有的输出都有$a_{22}$的贡献，所以：

$$
\frac{\partial J}{\partial a_{22}}=\frac{\partial J}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{22}}+\frac{\partial J}{\partial z_{12}} \frac{\partial z_{12}}{\partial a_{22}}+\frac{\partial J}{\partial z_{21}} \frac{\partial z_{21}}{\partial a_{22}}+\frac{\partial J}{\partial z_{22}} \frac{\partial z_{22}}{\partial a_{22}} 
$$
$$
=\delta_{z11} \cdot w_{22} + \delta_{z12} \cdot w_{21} + \delta_{z21} \cdot w_{12} + \delta_{z22} \cdot w_{11} \tag{7}
$$

同理可得所有$a$的梯度。

观察公式7中的$w$的顺序，貌似是把原始的卷积核旋转了180度，再与传入误差项做卷积操作，即可得到所有元素的误差项。而公式5和公式6并不完备，是因为二者处于角落，这和卷积正向计算中的padding是相同的现象。因此，我们把传入的误差矩阵Delta-In做一个zero padding，再乘以旋转180度的卷积核，就是要传出的误差矩阵Delta-Out，如图17-22所示。

![19](19.png)

图19 卷积运算中的误差反向传播

最后可以统一成为一个简洁的公式：

$$\delta_{out} = \delta_{in} * W^{rot180} \tag{8}$$

这个误差矩阵可以继续回传到下一层。

- 当Weights是$3\times 3$时，$\delta_{in}$需要padding=2，即加2圈0，才能和Weights卷积后，得到正确尺寸的$\delta_{out}$
- 当Weights是$5\times 5$时，$\delta_{in}$需要padding=4，即加4圈0，才能和Weights卷积后，得到正确尺寸的$\delta_{out}$
- 以此类推：当Weights是$N\times N$时，$\delta_{in}$需要padding=N-1，即加N-1圈0

举例：

正向时stride=1：$A^{(10 \times 8)}*W^{(5 \times 5)}=Z^{(6 \times 4)}$

反向时，$\delta_z^{(6 \times 4)} + 4 padding = \delta_z^{(14 \times 12)}$

然后：$\delta_z^{(14 \times 12)} * W^{rot180(5 \times 5)}= \delta_a^{(10 \times 8)}$

## 17.3 池化层

### 常用池化方法

池化 pooling，又称为下采样，downstream sampling or sub-sampling。

池化方法分为两种，一种是最大值池化 Max Pooling，一种是平均值池化 Mean/Average Pooling。如图21所示。

![21](21.png)

图21 池化

- 最大值池化，是取当前池化视野中所有元素的最大值，输出到下一层特征图中。
- 平均值池化，是取当前池化视野中所有元素的平均值，输出到下一层特征图中。

其目的是：

- 扩大视野：就如同先从近处看一张图片，然后离远一些再看同一张图片，有些细节就会被忽略
- 降维：在保留图片局部特征的前提下，使得图片更小，更易于计算
- 平移不变性，轻微扰动不会影响输出：比如上图中最大值池化的4，即使向右偏一个像素，其输出值仍为4
- 维持同尺寸图片，便于后端处理：假设输入的图片不是一样大小的，就需要用池化来转换成同尺寸图片

一般我们都使用最大值池化。

###  池化的其它方式

在上面的例子中，我们使用了size=2x2，stride=2的模式，这是常用的模式，即步长与池化尺寸相同。

我们很少使用步长值与池化尺寸不同的配置，所以只是提一下，如图22。

![22](22.png)

图22 步长为1的池化

上图是stride=1, size=2x2的情况，可以看到，右侧的结果中，有一大堆的3和4，基本分不开了，所以其池化效果并不好。

假设输入图片的形状是 $W_1 \times H_1 \times D_1$，其中W是图片宽度，H是图片高度，D是图片深度（多个图层），F是池化的视野（正方形），S是池化的步长，则输出图片的形状是：

$$
\begin{cases}
W_2 = (W_1 - F)/S + 1 \\\\
H_2 = (H_1 - F)/S + 1 \\\\
D_2 = D_1
\end{cases}
$$

池化层不会改变图片的深度，即D值前后相同。

###  池化层的训练

我们假设图23中，$[[1,2],[3,4]]$是上一层网络回传的残差，那么：

- 对于最大值池化，残差值会回传到当初最大值的位置上，而其它三个位置的残差都是0。
- 对于平均值池化，残差值会平均到原始的4个位置上。

![23](23.png)
图23 平均池化与最大池化
![24](24.png)

图24 池化层反向传播的示例

## 总结
卷积和池化的区别
卷积：
    1.Description: 和全连接的DNN相比, 卷积的网络参数大大减少, 因此连接权重和神经元个数的比重大大提升, 模型的计算量减少.
  2. 依据: 图像中往往有大量区域的构造类似, 因此全连接中有大量的连接的意义相同, 造成计算上的浪费.
  3. return: 最终得到训练好的卷积核.

池化:
  1. Description: 提取每层的特征, 传入下一层, 最常用的是max_pooling.
  2. 依据: 如: 最大池化, 待池化的层数据反映了某些特征的值, 最大池化也就是提取出当前区域中特征值最大, 通常即特征最显著的特征值, 传入下一层.
  3. return: 不返回任何存在

卷积神经网络的优点是：卷积神经网络CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，避免了显式的特征抽取，而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂度。



