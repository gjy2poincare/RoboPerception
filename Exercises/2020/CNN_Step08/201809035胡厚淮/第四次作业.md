## 第四次作业
### 时间：2020年12月1日
### CNN
### 前期准备工作
软件安装<br>
按下windows+R，输入cmd进入powershell<br>
第一行输入>pip install opencv-python<br>
运行结束后输入>pip install opencv-contrib-python<br>
运行结束后输入>pip install numba<br>
至此函数包已经安装完了，但要运行程序还需再VS-code的终端上将路径打开到需要运行的代码路径。然后试着运行程序就行了。

#### 一、卷积神经网络原理
##### 1.1、CNN定义
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一 [1-2]  。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）”.
##### 1.2、CNN能力
卷积神经网络（CNN，Convolutional Neural Net)是神经网络的类型之一，在图像识别和分类领域中取得了非常好的效果，比如识别人脸、物体、交通标识等，这就为机器人、自动驾驶等应用提供了坚实的技术基础。
##### 1.3、CNN的结构
一般结构如图：<br>
![](./images/4.1.png)<br>
- 输入层（Input Layer）：主要是对原始的图像数据进行预处理
- 卷积层（Convolution Layers）：可以看作是输入样本和卷积核的内积运算。从前一层提取移位不变特征。即当输入数据是图像的时候，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，在CNN中，可以（有可能）正确理解图像等具有形状的数据。注：卷积层的输入和输出数据通常又被称为特征图（Feature Map）。卷积层的输入数据称为输入特征图（Input Feature Map），输出数据称为输出特征图（Output Feature Map）。
- 池化层（Pooling Layers）：作用是减小卷积层产生的特征图尺寸。将前一层的多个单元的激活组合为一个单元。池化是缩小高、长方向上的空间的运算，通常减小一半。
- 全连接层（Fully Connected Layers）：收集空间扩散信息
- 输出层（Output Layer）：选择类
##### 1.4、CNN的特征
![](./images/4.2.png)
- 平移不变性
对于A的平移结果B，他们的卷积结果特征值是一样的，但特征值位于不同位置。
- 旋转不变性
对于一些图而言，小角度的旋转最后的特征值不变。但很多情况都不满足条件如6和9。
- 尺度不变性
对于A和D，CNN使用不同的卷积视野能看到一样照片。
#### 二、卷积的前向计算
##### 单输入多输出
![](./images/4.3.png)<br>
原始输入是一维的图片，但是我们可以用多个卷积核分别对其计算，从而得到多个特征输出。
 ##### 多输入单输出
![](./images/4.4.png)
虽然输入图片是多个通道的，或者说是三维的，但是在相同数量的过滤器的计算后，相加在一起的结果是一个通道，即2维数据，所以称为降维。这当然简化了对多通道数据的计算难度，但同时也会损失多通道数据自带的颜色信息。
##### 多入多出的同维卷积
![](./images/4.5.png)
第一个过滤器Filter-1为棕色所示，它有三卷积核(Kernal)，命名为Kernal-1，Keanrl-2，Kernal-3，分别在红绿蓝三个输入通道上进行卷积操作，生成三个2x2的输出Feature-1,n。然后三个Feature-1,n相加，并再加上b1偏移值，形成最后的棕色输出Result-1。

对于灰色的过滤器Filter-2也是一样，先生成三个Feature-2,n，然后相加再加b2，最后得到Result-2。

之所以Feature-m,n还用红绿蓝三色表示，是因为在此时，它们还保留着红绿蓝三种色彩的各自的信息，一旦相加后得到Result，这种信息就丢失了。
##### 卷积前向计算代码实现
设计方案(以三维卷积为例)：
- 预先设计好输出特征图的数量
- 每个特征图都和过滤器Filter对应
- 过滤器内部有一个或多个卷积核Kernal
- 每个过滤器只有一个偏置Bias
- 卷积核的矩阵格式一般为正方形奇数如3 * 3 , 5 * 5
- 使用定步长stride
- 考虑填充padding
- 输出矩阵的边长计算公式：
$$
H_{Output}= {H_{Input} - H_{Kernal} + 2Padding \over Stride} + 1
$$
代码分析：<br>

```Python
class ConvWeightsBias(WeightsBias_2_1):
    def __init__(self, output_c, input_c, filter_h, filter_w, init_method, optimizer_name, eta):
        self.FilterCount = output_c
        self.KernalCount = input_c
        self.KernalHeight = filter_h
        self.KernalWidth = filter_w
        ...

    def Initialize(self, folder, name, create_new):
        self.WBShape = (self.FilterCount, self.KernalCount, self.KernalHeight, self.KernalWidth)        
        ...
## 这个函数里面重写了系统初始化函数用于创建卷积核和初始化卷积核，整个类是继承WeightsBias_2_1。这里设计卷积核的高为filter_h,宽为filter_w....
```
```Python
class ConvLayer(CLayer):
    def forward(self, x, train=True):
        self.x = x
        self.batch_size = self.x.shape[0]
        # 如果有必要的话，先对输入矩阵做padding
        if self.padding > 0:
            self.padded = np.pad(...)
        else:
            self.padded = self.x
        #end if
        self.z = conv_4d(...)
        return self.z
```
```python
def conv_4d(x, weights, bias, out_h, out_w, stride=1):
    batch_size = x.shape[0]
    input_channel = x.shape[1]
    output_channel = weights.shape[0]
    filter_height = weights.shape[2]
    filter_width = weights.shape[3]
    rs = np.zeros((batch_size, num_output_channel, out_h, out_w))

    for bs in range(batch_size): ## 1. 批量数据循环（第一维）：对每个样本进行
        for oc in range(output_channel):## 输出通道循环（第二维）
            rs[bs,oc] += bias[oc]
            for ic in range(input_channel):##  输入通道循环：
                for i in range(out_h): ## 输出图像纵坐标循环
                    for j in range(out_w):  ## 输出图像横坐标循环
                        ii = i * stride
                        jj = j * stride
                        ## 循环4和5完成对输出图像的每个点的遍历，在下面的子循环中计算并填充值；
                        for fh in range(filter_height):   ##卷积核纵向循环（第三维）
                            for fw in range(filter_width):  ## 卷积核横向循环（第四维）：
                                rs[bs,oc,i,j] += x[bs,ic,fh+ii,fw+jj] * weights[oc,ic,fh,fw]
                                ##循环完成卷积核与输入图像的卷积计算，并保存到循环4和5指定的输出图像的点上。
```


#### 三、卷积的反向传播原理
#### 四、池化
#### 五、经典卷积神经网络
#### 六、卷积神经网络的运用
