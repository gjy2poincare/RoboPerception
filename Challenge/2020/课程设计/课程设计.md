# 课程设计  基于OpenCV的车道线检测

## 一、问题定义

车道线的分类：
按照道路交通标线的功能划分为：指示标线、警告标线和禁止标线。

按标划方法可分为：白色虚线、白色实线、黄色虚线、黄色实线、双白虚线、双白实线、双黄虚线和双黄实线等。

按作用又可分为：车行道中心线、车道分界线、停止线、减速让行线、人行横道线、导流线、导向箭头和左转弯导线等。

车道检测的目标：
1. 车道形状，包括宽度、曲率等几何参数

2. 车辆在车道中的位置，包括横向偏移量，车辆与道路的夹角（偏航角）

车道检测与跟踪一般分为以下几个部分：
1. 车辆、道路、相机模型

2. 道路特征提取

3. 道路参数计算，如曲率，

4. 车道跟踪

车道线检测，就是在汽车行驶时通过汽车前方的摄像头捕捉到地面的图像（视频），通过Opencv对图片（视频）进行一系列处理后，可以在图片（视频）中标注出车道线，然后以此作为汽车自动驾驶时的安全行驶范围。由此可见车道线检测是无人驾驶中最为重要且基础的一环。在无人车自动行驶的过程中，实时获取路面视频流，然后对视频进行处理，标注出可行驶的路线。

车道线检测是自动驾驶领域的关键技术之一，它被广泛地应用于辅助驾驶系统。即使现有基于深度学习的车道线检测方案，但是由于用于自动驾驶领域的深度学习应用在实时性和鲁棒性上的缺陷，实际应用中大规模部署的仍然是传统的计算机视觉的方案，并且大部分深度学习方案都依赖于传统计算机视觉技术对图像进行预处理。

原理图：
![](./picture/8.PNG)

## 二、算法设计

关于车道线检测，本次实验只是对图片进行处理，因此省去了不少步骤，本次实验展示了如何将图片的车道线进行检测并标注出来，之后调用pymovie将视频中的每一个目标视频，分割为一帧一帧的图片，然后针对每一个图片进行车道线的识别，最终将每一帧再合成为一个输出视频，整个课程不止针对简单的车道线，还考虑了在阴影条件下，在车道线有弯道时，以及车道线并不完整有部分车道线消失时依旧能够完成车道线的检测。
大体上整个识别的过程可以分为以下几个步骤：

1.加载图片，将其变为由RGB表示的数组
2.对图片进行灰度处理，并去除噪点
3.通过opencv自带函数实现边缘检测（sobel算子，canny算子，prewitt算子）
4.对检测后的图像裁剪出车道线
5.通过霍夫变换将车道线画出
6.将检测到车道线加载到原图像中

车道线检测，需要完成以下功能：

1.图像裁剪：通过设定图像ROI区域，拷贝图像获得裁剪图像
2.反透视变换：用的是室外采集到的视频，没有对应的变换矩阵。所以建立二维坐标，通过四点映射的方法计算矩阵，进行反透视变化。后因ROI区域的设置易造成变换矩阵获取困难和插值得到的透视图效果不理想，故没应用
3.二值化：先变化为灰度图，然后设定阈值直接变成二值化图像。
4.形态学滤波：对二值化图像进行腐蚀，去除噪点，然后对图像进行膨胀，弥补对车道线的腐蚀。
5.边缘检测：canny变化、sobel变化和laplacian变化中选择了效果比较好的canny变化，三者在代码中均可以使用，canny变化效果稍微好一点。
6.直线检测：实现了两种方法 1>使用opencv库封装好的霍夫直线检测函数，在原图对应区域用红线描出车道线 2>自己写一种直线检测，在头文件中，遍历ROI区域进行特定角度范围的直线检测。两种方法均可在视频中体现，第一种方法运行效率较快。
7.按键控制：空格暂停，其余键退出，方便调试和截图。

### 2.1 通过opencv中的各种滤波器进行边缘检测

边缘检测的一般标准包括：
① 以低的错误率检测边缘——尽可能准确的捕获图像中尽可能多的边缘

② 检测到的边缘应精确定位在真实边缘的中心。

③ 图像中给定的边缘应只被标记一次，并且在可能的情况下，图像的噪声不应产生假的边缘。边缘只要一个精确的点宽度。

边缘检测也就是对图像求导数的过程，图像的边缘也就是一阶导数出现极值点，或者二阶导数为零的情况。在opencv中有3种边缘检测算子，一阶导数Roberts、Sobel、Prewitt。二阶导数Laplacian、Log/Marr、(Kirsch、Nevitia)，非微分边缘检测算子：Canny。

通过以上算子，来对整个图像求解梯度，我们可以看出在Sobel图像中可以提取出明显的车道线边缘，以及汽车边缘，周围较为明显景物的边缘。Sobel算子对于灰度渐变和噪声较多的图像有着较好的处理效果，Sobel算子对于边缘定位比较准确。相比之下Reberts，Prewitt、Laplacian都是跟Sobel算子不同，针对的图像也不同，导致最后得到的效果不同，而Canny（）是内部使用了Sobel算子，对每个像素点计算出梯度强度和方向之后，通过应用非极大值（Non-Maximum Suppression）抑制，以消除边缘检测带来的杂散响应。

为了尽可能减少噪声对边缘检测结果的影响，所以必须滤除噪声以防止由噪声引起的错误检测。为了平滑图像，使用高斯滤波器与图像进行卷积，该步骤将平滑图像，以减少边缘检测器上明显的噪声影响。

使用Non-Maximum Suppression（非极大值抑制）消除边缘检测带来的杂散响应；
如果直接把梯度作为边缘的话，将得到一个粗边缘的图像，在图像边缘区域，其附近梯度值往往都很大，这不满足上面提到的准则（最小响应标准），我们希望得到定位准确的单像素的边缘，所以将每个像素点的梯度与其梯度方向上的相邻像素比较，如果不是极大值，将其置0，否则置为某一不大于255的数，非最大值抑制能帮助保留局部最大梯度而抑制所有其他梯度值。这意味着只保留了梯度变化中最锐利的位置

Canny滤波器根据选择不同的low_threshold，最终获取的图像也是不同，这种方法可以把图片中距离较远，树木车俩等较为模糊的物体，最后的由于阈值取的较高而被抑制，最终图像也就不再包含其边缘。

### 2.2 对边缘检测后的图像进行截取(获取到车道线的边缘即可)

通过上面的几个操作，我们可以看到车道线已经能够检测出来，然而仅靠Canny函数的阈值来调整图像，是无法获得单独的车道线的，当阈值调整至135，左侧车道线就会少一部分，但是左侧的边缘仍然无法消除。

这时我们可以考虑对该边缘线进行筛选，假设无人车上的 摄像头角度不变，那么在保证车辆行驶在规定车道线内的前提下，相对于车来说，前方的车道线的位置是固定的（直线）。左右两条车道线以相对的斜率，在前方的某点相交。（通过基于透视的图像矫正方法，可以将我们所看到的两条斜直线，转为两条相交直线）。 因此车道线多在图片的下面的三角区域之中，根据本图像的大小为（540，960），而我们的图像是一个二维数组，通过标定一个区域

将所有该区域内的车道线边缘检测结果保留，其他的令其像素点值为0。整个过程为将标定的范围标为mask（初始为与原图像shape相同的全0数组），将该区域内标为255，然后将mask与原图像进行与操作，这样就可以只保留下区域内的结果。其他区域像素点值变为0，也就变成纯黑色。

### 2.3 通过霍夫变换车道线进行补全并绘制至原图像中

在将图像进行截取之后，据我们想要的最终结果已经很接近了，然而由于有的车道线是虚线形式的，我们所获得的图像也就会成为类似矩形的分段式的图像。在opencv中最常用的就是霍夫变换（Hough Lines()）中最常用的就要将其转化为左右两条直线。

霍夫变换的原理：

对于一条直线，我们可以将直线上的点用极坐标来表示出来。因此一条直线也就被转换为θ角度内(r,θ)的集合，也被称为霍夫空间。

整条直线也就被映射为以θ为横坐标，r为纵坐标的曲线，同一条直线上的点就会表现为多个曲线的交点。霍夫变换，就是根据相交曲线的数量，来判断是否是直线，假如只有3条曲线相交，它可能是图像恰好不同位置的交点而已（其实也不一定，因为在进行canny的边缘检测后，我们又对区域进行截取，已经能保证区域中95%以上都是车道线内容，当可能水平直线中，左右车道线中有部分点处于同一直线，就会导致结果中出现垂直的变换结果）因此对于参数threshold我们还是设置的多一点比较好。（也不能太多，不然，对于虚线情况下的车道线说不定数量也不够，这样就少线了，可能导致之后的拟合出现误差）

做到这，基础的对图片进行车道线检测就算是做到尾声了，接下来就是只要将该车道线绘制到原图像上即可，最终的效果图如下，我们可以清楚的看到我们找到车道线与实际车道线的重合。

## 三、源代码

#include <opencv2/opencv.hpp>
#include <iostream>
#include <cmath>

using namespace cv;
using namespace std;

/**
**1、读取视频
**2、二值化
**3、轮廓发现
**4、轮廓分析、面积就算，角度分析
**5、直线拟合
**6、画出直线
**
*/

Point left_line[2];
Point right_line[2];

void process(Mat& frame, Point* left_line, Point* right_line);
Mat fitLines(Mat& image, Point* left_line, Point* right_line);

int main(int argc, char** argv) {
	//读取视频
	VideoCapture capture("2.mp4");

	int height = capture.get(CAP_PROP_FRAME_HEIGHT);
	int width = capture.get(CAP_PROP_FRAME_WIDTH);
	int count = capture.get(CAP_PROP_FRAME_COUNT);
	int fps = capture.get(CAP_PROP_FPS);
	//初始化

	left_line[0] = Point(0, 0);

	left_line[1] = Point(0, 0);

	right_line[0] = Point(0, 0);

	right_line[1] = Point(0, 0);

	cout << height << "       " << width << "       " << count << "       " << fps << endl;

	//循环读取视频
	Mat frame;
	while (true) {
		int ret = capture.read(frame);
		if (!ret) {
			break;
		}
		imshow("input", frame);
		process(frame, left_line, right_line);

		char c = waitKey(5);
		if (c == 27) {
			break;
		}


	}

}

void process(Mat& frame, Point* left_line, Point* right_line) {
	Mat gray, binary;
	/**灰度化*/
	cvtColor(frame, gray, COLOR_BGR2GRAY);

	//threshold(gray, binary, );
	//边缘检测
	Canny(gray, binary, 150, 200);
	//imshow("Canny", binary);
	for (size_t i = 0; i < (gray.rows / 2 + 40); i++) {
		for (size_t j = 0; j < gray.cols; j++)
		{
			binary.at<uchar>(i, j) = 0;
		}
	}
	imshow("binary", binary);

	//寻找轮廓
	vector<vector<Point>> contours;
	findContours(binary, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);

	Mat out_image = Mat::zeros(gray.size(), gray.type());

	for (int i = 0; i < contours.size(); i++)
	{

		//计算面积与周长
		double length = arcLength(contours[i], true);
		double area = contourArea(contours[i]);
		//cout << "周长 length:" << length << endl;
		//cout << "面积 area:" << area << endl;

		//外部矩形边界
		Rect rect = boundingRect(contours[i]);
		int h = gray.rows - 50;

		//轮廓分析：
		if (length < 5.0 || area < 10.0) {
			continue;
		}
		if (rect.y > h) {
			continue;
		}

		//最小包围矩形
		RotatedRect rrt = minAreaRect(contours[i]);
		//关于角度问题：https://blog.csdn.net/weixin_41887615/article/details/91411086


		//cout << "最小包围矩形 angle:" << rrt.angle << endl;

		double angle = abs(rrt.angle);

		//angle < 50.0 || angle>89.0

		if (angle < 20.0 || angle>84.0) {

			continue;

		}


		if (contours[i].size() > 5) {
			//用椭圆拟合
			RotatedRect errt = fitEllipse(contours[i]);
			//cout << "用椭圆拟合err.angle:" << errt.angle << endl;

			if ((errt.angle < 5.0) || (errt.angle > 160.0))
			{
				if (80.0 < errt.angle && errt.angle < 100.0) {
					continue;
				}

			}
		}


		//cout << "开始绘制：" << endl;
		drawContours(out_image, contours, i, Scalar(255), 2, 8);
		imshow("out_image", out_image);

	}
	Mat result = fitLines(out_image, left_line, right_line);
	imshow("result", result);

	Mat dst;
	addWeighted(frame, 0.8, result, 0.5, 0, dst);
	imshow("lane-lines", dst);

}

//直线拟合
Mat fitLines(Mat& image, Point* left_line, Point* right_line) {
	int height = image.rows;
	int width = image.cols;

	Mat out = Mat::zeros(image.size(), CV_8UC3);

	int cx = width / 2;
	int cy = height / 2;

	vector<Point> left_pts;
	vector<Point> right_pts;
	Vec4f left;


	for (int i = 100; i < (cx - 10); i++)
	{
		for (int j = cy; j < height; j++)
		{
			int pv = image.at<uchar>(j, i);
			if (pv == 255)
			{
				left_pts.push_back(Point(i, j));
			}
		}
	}

	for (int i = cx; i < (width - 20); i++)
	{
		for (int j = cy; j < height; j++)
		{
			int pv = image.at<uchar>(j, i);
			if (pv == 255)
			{
				right_pts.push_back(Point(i, j));
			}
		}
	}

	if (left_pts.size() > 2)
	{
		fitLine(left_pts, left, DIST_L1, 0, 0.01, 0.01);

		double k1 = left[1] / left[0];
		double step = left[3] - k1 * left[2];

		int x1 = int((height - step) / k1);
		int y2 = int((cx - 25) * k1 + step);

		Point left_spot_1 = Point(x1, height);
		Point left_spot_end = Point((cx - 25), y2);


		line(out, left_spot_1, left_spot_end, Scalar(0, 0, 255), 8, 8, 0);
		left_line[0] = left_spot_1;
		left_line[1] = left_spot_end;

	}
	else
	{
		line(out, left_line[0], left_line[1], Scalar(0, 0, 255), 8, 8, 0);
	}




	if (right_pts.size() > 2)
	{

		Point spot_1 = right_pts[0];
		Point spot_end = right_pts[right_pts.size() - 1];

		int x1 = spot_1.x;

		int y1 = spot_1.y;

		int x2 = spot_end.x;
		int y2 = spot_end.y;


		line(out, spot_1, spot_end, Scalar(0, 0, 255), 8, 8, 0);
		right_line[0] = spot_1;
		right_line[1] = spot_end;

	}
	else
	{
		line(out, right_line[0], right_line[1], Scalar(0, 0, 255), 8, 8, 0);
	}

	return out;
}

## 四、实验结果
![](./picture/1.PNG)
![](./picture/2.PNG)
![](./picture/3.PNG)
![](./picture/4.PNG)
![](./picture/5.PNG)
![](./picture/6.PNG)
![](./picture/7.PNG)


## 五、实验总结

在本次实验中，主要通过高斯滤波，Canny边缘检测，霍夫变换完成大部分内容，本次实验也只是对单一图片进行识别，同时道路还是直线，就更为的简单，相比于视频流的输入，面对不同路况以及弯曲的车道线，以及将车道线进行透明转换，该实验也只是入门中的入门。通过这次动手也加深了对高斯模糊，高通滤波，Sobel算子，Canny中阈值的变换，霍夫变换笛卡尔坐标与极坐标的转换这些理论进行了各种实践与尝试。

还有一个问题就是，对于左右两侧的车道线的识别，当无人驾驶的汽车进行变道时，只靠一个摄像头捕捉肯定是不够的，这时候单用这种算法一定会出现错误，应该只能在两侧都捕捉路面信息，与本次实验不同，只是在截取车道线时，不是再选一个正下方类似三角形的形状，二是左（右）侧类似平行四边形的形状。这样就能保证获取周围车道线的信息，为变更车道做准备。

## 六、心得体会

opencv是非常强大的工具。opencv提供了一个相当广阔的平台。通过教材上的内容和网上的资料学会了很多opencv的知识。我暂且分为以下几点。

-1.对于一个视频，世界坐标系和图像坐标的不同，怎样从空间结构变为平面结构。用矩阵和图像结构存储的不同，颜色信息如何保存（double型数据），彩色图和二值图如何转换（opencv中一些函数是基于二值图完成的，如果要保留颜色信息，可以通过分通道操作来完成，如RGB图分解为三个单通道图来分别操作），摄像机视角和鸟瞰视角如何切换（反透视变换矩阵）。怎样建立模型存储图像和视频中的信息（对不同坐标系的理解和转换方法）。

-2.如何对视频进行预处理（模糊，腐蚀，膨胀等等），每种操作opencv都提高一个完善的函数来实现，可以通过调节函数参数来得到自己想要的效果。减少画面中的障碍和杂质，使得画面更利于接下来检测的工作（无论是车道线检测还是行人、车辆检测，进行恰当的预处理会大大有利于之后的特征提取算法的运用）。

-3.ROI的概念，当视频画面中我们只需要对某一区域进行扫描和判断时可以对它设置恰当的ROI，当我们需要回到完整画面处理时可以释放ROI。同时注意到如果在设置ROI前进行了矩阵变换，则进行逆变换时必须回到进行矩阵变换当时的那个ROI区域。否则画面的比例是不一致，做不出理想的效果的。

-4.同时，opencv还可以完成丰富的UI设计，包括滑动条，标题，颜色等都是可以自定义的，通过调节这些可以提高视觉效果，尽管这些并非是算法范围内的事情，但是一个好的界面和布局，能为程序员编程和debug带来更多乐趣，也能为完成项目后，使用作品的用户带来更友好的用户体验。用opencv来完成一个小型的Photoshop软件也是可能的（可以调整图像亮度，对比度，剪裁，模糊图像，腐蚀膨胀等效果可以改造为滤镜效果），这是非常棒的事情，希望接下来能继续完善和做出一些更有趣的东西。